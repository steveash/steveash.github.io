---
layout: slides
title: Intro to Structured Prediction in 3 Acts
description: Introduction to structured prediction showing 3 methods used to solve it in historical order
theme: sky
transition: none
permalink: /talks/2016-structured-prediction
---

<section data-markdown>
## Intro to Structured Prediction in 3 Acts

Steve Ash

[www.manycupsofcoffee.com](www.manycupsofcoffee.com)
</section>

<section data-markdown>
### About Me

* BA, MS, PhD (in Spring '17) Computer Science
* 10 years building banking and healthcare software
* 5 years leading a "skunkworks" R&amp;D team
</section>

<section data-markdown>
### Agenda

* What is Structured Prediction?
* **Act 1**: How to solve this problem in 1996? 
    * _generative models_

* **Act 2**: How to solve this problem in 2006? 
    * _discriminative models_

* **Act 3**: How to solve this problem in 2016?
    * _recurrent neural networks_

</section>

<section data-markdown>
### What is Structured Prediction

Common machine learning tasks:
* **Classification**: input data to fixed number of output classes/categories
   * Is this credit card transaction fraudulent?
   * Does this patient have liver disease?

* **Regression**: input data to continuous output
   * Prospective student's future GPA based on information from his/her college application

What about classification over a _sequence_ of information?
</section>
<section data-markdown>
### What is Structured Prediction

Assign part-of-speech tags to sequence of words: 

"Steve plays piano sometimes"

![Parts of Speech]({{ "/talks/pred-steve-pos-1.png" | prepend: site.baseurl }})

</section>
<section data-markdown>

### What is Structured Prediction

The sequence of words &mdash; i.e. the _structure_ &mdash; affects the part-of-speech. We need to incorporate this structure into the predictive method that we want to use in order to improve our accuracy. 

Without the structure we can't disambiguate words that have more than one part-of-speech:

> "Will Bob will the will to Will?"

</section>
<section data-markdown>
### Quick background

In probabilistic modelling we model our inputs and outputs as _random variables_.
* Coin flip `$P(X=\textrm{heads})=0.5$`

In the part-of-speech example, we had a sequence of words and a seqence of POS tags.

* `$X_t$` = the word at time `$t$` 
* `$Y_t$` = the predicted POS tag at time `$t$`
    * `$P(Y_t=\textrm{NOUN}) = 0.1232$`

</section>
<section data-markdown>
### Quick background

Joint probability distribution
`$$P(X=\textrm{'will'}, Y=\textrm{NOUN})$$`

Conditional probability distribution
`$$P(Y=\textrm{NOUN} | X=\textrm{'will'})$$`

Relationship: `$P(X, Y)=P(X | Y) \times P(Y)$`

Bayes Rule: `$P(Y | X)=\frac{P(X | Y) \times P(Y)}{P(X)}$`
</section>
<section>
  <h3>Quick Background</h3>
  <p>Joint distribution knows everything about $P(X, Y)$</p>
  <style type="text/css">
  table.tableizer-table {
    border: 1px solid #CCC; 
  } 
  .tableizer-table td {
    padding: 4px;
    margin: 3px;
    border: 1px solid #CCC;
  }
  td.tableizer-firstrow,.tableizer-firstrow th {
    background-color: #166CC0; 
    color: #FFF;
    font-weight: bold;
  }
</style>
<table class="tableizer-table">
<thead><tr class="tableizer-firstrow"><th></th><th>NOUN</th><th>VERB</th><th>ADJ</th></tr></thead><tbody>
 <tr><td class="tableizer-firstrow">dog</td><td>13423</td><td>34</td><td>0</td></tr>
 <tr><td class="tableizer-firstrow">walk</td><td>4300</td><td>7400</td><td>0</td></tr>
 <tr><td class="tableizer-firstrow">cat</td><td>12002</td><td>12</td><td>0</td></tr>
 <tr><td class="tableizer-firstrow">short</td><td>904</td><td>3123</td><td>4339</td></tr>
</tbody></table>
</section>

<section data-markdown>
# Act 1
Generative Models
</section>

<section data-markdown>
### Hidden Markov Models

**Idea:**
* Inputs are **observed** values, outputs are **hidden** states
* These hidden states can be modeled as nodes in an automata
* Each hidden state **emits** observed values
* Each state can **transition** to other states
* A sequence of outputs is a **path** through this automata

![HMM states]({{ "/talks/pred-hmm-1.png" | prepend: site.baseurl }})
</section>

<section data-markdown>
### Hidden Markov Models

Models the real world process as:
* Select a starting state (or have a single starting state)
* **transition** to a state in the machine
* **emit** an observed value
* **transition** to this next state
* ...

|               | `$t = 0$` | `$t = 1$` | `$t = 2$` | `$t = 3$` |
|---------------|-----------|-----------|-----------|-----------|
| Hidden State  | NOUN      | VERB      | NOUN      | ADVERB    |
| Emitted Value | Steve     | plays     | piano     | sometimes |

</section>
<section data-markdown>
### Hidden Markov Models

* `$A_{ij}$` = the **transition** probability of going from state `$Y_t$` to `$Y_{t+1}$`
* `$B_m(k)$` = the **emission** probability of observing $m$ from state $k$
* `$\pi_i$` = the **initial** probability of starting the sequence from state $i$

We see `$\textbf{O}_t$` is `$\textbf{O}_0 = \textrm{Steve}, $\textbf{O}_1 = \textrm{plays},\dots$`

Hidden states `$\textbf{Q}_t$` is `$\textbf{Q}_0 = \textrm{NOUN}, $\textbf{Q}_1 = \textrm{VERB},\dots$`
</section>