---
layout: post
title: Using ELK (elasticsearch + logstash + kibana) to aggregate cassandra and spark
  logs
date: '2015-06-07T19:36:00.000-05:00'
author: Steve Ash
tags: 
modified_time: '2015-06-07T19:36:13.836-05:00'
thumbnail: http://4.bp.blogspot.com/-4dKM2Qfv2Xo/VXStShbVf-I/AAAAAAAALPQ/lYpHb2IC0Kk/s72-c/spark_cass_elk.png
blogger_id: tag:blogger.com,1999:blog-4471109663192942674.post-2390904368964560354
blogger_orig_url: http://blog.manycupsofcoffee.com/2015/06/using-elk-elasticsearch-logstash-kibana.html
---

On one of my current projects we are using Cassandra and Spark Streaming to do some somewhat-close-to-real time analytics.  The good folks at <a href="http://www.datastax.com/" target="_blank">Datastax</a> have built a commercial packaging (Datastax Enterprise, aka DSE) of Cassandra and Spark that allow you to get this stack up and running with relatively few headaches.  One thing the Datastax offering does not include is a way to aggregate logs across all of these components. There are quite a few processes running across the cluster, each producing log files.  Additionally, spark creates log directories for each application and driver program, each with their own logs.  Between the high count of log files and the fact that work happens on all the nodes different each time depending on how the data gets partitioned- it can become a huge time sink to hunt around and grep for the log messages that you care about.<br /><br />Enter the ELK stack.  ELK is made up of three products: <br /><br /><ul><li><b><a href="https://www.elastic.co/products/elasticsearch" target="_blank">Elasticsearch</a></b> - a distributed indexing and search platform. &nbsp;It provides a REST interface on top of a fancy distributed, highly available full text and semi-structured text searching system. &nbsp;It uses Lucene internally for the inverted index and searching.</li><li><b><a href="https://www.elastic.co/products/logstash" target="_blank">Logstash</a></b> - log aggregator and processor. &nbsp;This can take log feeds from lots of different sources, filter and mutate them, and output them somewhere else (elasticsearch in this case). &nbsp;Logstash is the piece that needs to know about whatever structure you have in your log messages so that it can pull the structure out and send that semi-structured data to elasticsearch.&nbsp;</li><li><b><a href="https://www.elastic.co/products/kibana" target="_blank">Kibana</a></b> - web based interactive visualization and query tool. &nbsp;You can explore the raw data and turn it into fancy aggregate charts and build dashboards.</li></ul><div>All three of these are open-source, Apache 2 licensed projects built by <a href="https://www.elastic.co/" target="_blank">Elastic</a>, a company founded by the folks that wrote Elasticsearch and Lucene. &nbsp;They have all of the training, professional services, and production support subscriptions, and a stable of products with confusing names that you aren't quite sure if you need or not...</div><div><br /></div><div>So how does this look at a high level? Spark and Cassandra run co-located on the same boxes. &nbsp;This is by design so that your Spark jobs can use RDDs that use a partitioning scheme that is aware of Cassandra's ring topology. &nbsp;This can minimize over-the-wire data shuffles, improving performance. &nbsp;This diagram shows at a high level where each of these processes sit in this distributed environment:</div><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-4dKM2Qfv2Xo/VXStShbVf-I/AAAAAAAALPQ/lYpHb2IC0Kk/s1600/spark_cass_elk.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="120" src="http://4.bp.blogspot.com/-4dKM2Qfv2Xo/VXStShbVf-I/AAAAAAAALPQ/lYpHb2IC0Kk/s400/spark_cass_elk.png" width="400" /></a></div><div class="separator" style="clear: both; text-align: left;">This only depicts two "analytics" nodes and one ELK node, but obviously you will have more of each. &nbsp;Each analytics node will be producing lots of logs. &nbsp;Spark writes logs to:</div><div class="separator" style="clear: both; text-align: left;"></div><ul><li>/var/log/spark/worker</li><li>/var/lib/spark/worker/worker-0/app-{somelongtimestamp}/{taskslot#}</li><li>/var/lib/spark/worker/worker-0/driver-{somelongtimestamp}</li></ul><br /><div class="separator" style="clear: both; text-align: left;">To collect all of these logs and forward, there is an agent process on each node called Logstash-Forwarder that is monitoring user specified folders for new log files and is shipping them over via TCP to the actual logstash server process running on the ELK node. &nbsp;Logstash receives these incoming feeds, parses them and sends them to elasticsearch. &nbsp;Kibana responds to my interactive queries and delegates all of the search work to elasticsearch. &nbsp;Kibana doesn't store any results internally or have its own indexes or anything.</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">Others have already done a good job <a href="https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-4-on-centos-7" target="_blank">explaining how to setup ELK</a> and <a href="https://www.timroes.de/2015/02/07/kibana-4-tutorial-part-1-introduction/" target="_blank">how to use Kibana</a>, and therefore I won't repeat all of that here. &nbsp;I will only highlight some of the differences, and share my Logstash configuration files that I had to create to handle the out-of-the-box log file formats for Cassandra and Spark (as packaged in DSE 4.7 at least).</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">I installed elasticsearch from the repo, which already created the systemd entries to run it as a service. &nbsp;Following the ELK setup link above, I created systemd entries for logstash and kibana. &nbsp;I also created a <a href="https://gist.github.com/steveash/82f5f3bd10b1a6eb7bcc#file-logstash-forwarder-service" target="_blank">systemd unit file for the logstash-forwarder</a> running on each analytics node. &nbsp;</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">The logstash-forwarder needs to be configured with all of the locations that spark and cassandra will put logfiles. &nbsp;It supports glob syntax, including recursive folder searches like "whatever/**/*.log", but I ended up not using that because it was duplicating some of the entries due to a wonky subfolder being created under the spark driver program log folder called cassandra_logging_unconfigured. &nbsp;My forwarder configuration picks up all of the output logs for the workers, the applications, and the driver and creates a different <b>type</b>&nbsp;for each: <b>spark-worker</b> for generic /var/log spark output, <b>spark-app</b> for app-* log folder, <b>spark-driver</b> for the driver programs (where most of the interesting logging happens). &nbsp;<a href="https://gist.github.com/steveash/82f5f3bd10b1a6eb7bcc#file-logstash-forwarder-conf" target="_blank">My logstash-forwarder.conf</a> is in the gist.</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">For logstash I setup a few files as a pipeline to handle incoming log feeds:</div><div class="separator" style="clear: both; text-align: left;"></div><ul><li><a href="https://gist.github.com/steveash/82f5f3bd10b1a6eb7bcc#file-00_input-conf" target="_blank">00_input.conf</a> - sets up the lumberjack (protocol the forwarder uses) port and configures certs&nbsp;</li><li><a href="https://gist.github.com/steveash/82f5f3bd10b1a6eb7bcc#file-01_cassandra_filter-conf" target="_blank">01_cassandra_filter.conf</a> - parses the logback formats that DSE delivers for cassandra. &nbsp;Im not sure if vanilla open-source cassandra uses the same by defualt or not. &nbsp;There are two formats between sometimes there is an extra value in here -- possibly from the logback equivalent of NDC.</li><li><a href="https://gist.github.com/steveash/82f5f3bd10b1a6eb7bcc#file-02_spark_filter-conf" target="_blank">02_spark_filter.conf</a> - parses the logback formats that DSE delivers for spark. &nbsp;I see there are two formats I get here as well. &nbsp;Sometimes with a line number, sometimes without.</li><li><a href="https://gist.github.com/steveash/82f5f3bd10b1a6eb7bcc#file-07_last_filter-conf" target="_blank">07_last_filter.conf</a> - this is a multiline filter that recognizes java stacktraces and causes them to stay with the original message</li><li><a href="https://gist.github.com/steveash/82f5f3bd10b1a6eb7bcc#file-10_output-conf" target="_blank">10_output.conf</a> - sends everything to elasticsearch</li></ul><br /><div class="separator" style="clear: both; text-align: left;">All of my configuration files are available through the links above <a href="https://gist.github.com/steveash/82f5f3bd10b1a6eb7bcc" target="_blank">in this gist</a>. &nbsp;Between the linked guides above and the configuration here that should get you going if you need to monitor cassandra and spark logs with ELK!</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;"><b>Quick tip:</b> While you're getting things configured and working, you might need to kill the currently indexed data and resend everything (so that it can reparse and re-index). The logstash-forwarder keeps a metadata file called <span style="font-family: Courier New, Courier, monospace;">.logstash-forwarder</span> in the working directory where you started the forwarder. &nbsp; If you want to kill all of the indexed data and resend everything, follow these steps:</div><div class="separator" style="clear: both; text-align: left;"></div><ol><li>Kill the logstash-forwarder: <br /><span style="font-family: Courier New, Courier, monospace;">sudo systemctl stop logstash-forwarder</span></li><li>Delete the logstash-forwarder metadata so it starts from scratch next time: <br /><span style="font-family: Courier New, Courier, monospace;">sudo rm /var/lib/logstash-forwarder/.logstash-forwarder</span></li><li>If you need to change any logstash configuration files, do that and then restart logstash: <br /><span style="font-family: Courier New, Courier, monospace;">sudo systemctl restart logstash</span></li><li>Delete your existing elastic search indexes (be careful with this!): <br /><span style="font-family: Courier New, Courier, monospace;">curl -XDELETE 'http://<elasticsearch-host>:9200/logstash-*'</elasticsearch-host></span></li><li>Start the logstash-forwarder again:<br /><span style="font-family: Courier New, Courier, monospace;">sudo systemctl start logstash-forwarder</span></li></ol>Also note that by default the logstash-forwarder will only pickup files less than 24 hours old. &nbsp;So if you're configuring ELK and playing with filters on stagnant data, make sure at least some files are touched recently so it will pick them up. &nbsp;Check out the log file in /var/log/logstash-forwarder to see if it's skipping particular entries. &nbsp;You can also run the logstash-forwarder with -verbose to see additional debug information.<div><br /></div><div><b>Quick tip:</b> use the wonderfully useful&nbsp;<a href="http://grokdebug.herokuapp.com/">http://grokdebug.herokuapp.com/</a>&nbsp;to test out your grow regex patterns to make sure they match and pull out the fields you want.<br /><br /><div><br /></div></div>